# Novel View Synthesis with Style Transfer via 3D feature embeddings
The advent of adversarial models has given rise to very powerful image synthesis. However, despite the rich depth in 2D image models, mathematical models of 3D environments have not been explored thoroughly. We propose to apply a hybrid generational model for style transfer upon an underlying 3D scene structure.

# Introduction
Recent years have seen progress in applying machine learning techniques to develop a 3D representation of a particular scene from 2D images. With the introduction of efficient 3D representations such as DeepVoxels, Scene Representation Networks (SRNs) and Neural Meshes, these neural networks are able to generate novel views of an object learned from a set of 2D images.

However, little work has been done in transforming these underlying 3D structures in meaningful ways. Can we, for instance, apply a new floral pattern to a 3D model of shape? We aim to explore the style transfer of a 2D image upon latent 3D models for novel view synthesis.
